% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
  \usepackage{amssymb}
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={MA8701 Advanced methods in statistical inference and learning},
  pdfauthor={Mette Langaas IMF/NTNU},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\title{MA8701 Advanced methods in statistical inference and learning}
\author{Mette Langaas IMF/NTNU}
\date{04 April, 2021}

\begin{document}
\frame{\titlepage}

\begin{frame}{Part 4: Explainable AI}
\protect\hypertarget{part-4-explainable-ai}{}
\begin{block}{Why a part on XAI?}
\protect\hypertarget{why-a-part-on-xai}{}
In \textbf{Part 1} we worked with \emph{interpretable} methods:

\begin{itemize}
\tightlist
\item
  linear regression (LS/MLE, ridge and lasso)
\item
  logistic regression (MLE, ridge and lasso)
\end{itemize}

By studying the estimated regression coefficients we could (to some
extent) explain what our fitted model could tell us about the data we
had analysed.

In \textbf{Part 2} we started by studying a classification and
regression tree, which is also an interpretable method.

See Chapter 4.1, 4.2 and 4.4 of Molnar (2019) (Chapther 4\_
Interpretable Models, 4.1 Linear regression, 4.2 Logistic regression and
4.4 Decision tree) on a discussion around what to report or plot from
interpretable methods.

In Part 2 we then moved on to different versions of ensemble methods
(bagging, random forest, xgboost, superlearner) - which are not
interpretable. In \textbf{Part 3} we studied artificial neural networks
(deep nets, recurrent nets, Bayesian nets) - again not interpretable
methods.

We may refer to the methods of Part 2 and 3 as \emph{black box} methods,
since in a prediction setting we would input an observation to the
fitted method and the method would output a prediction - but we would
not have a specific formula that we use to explain why the method gave
this prediction.

In many situations we would like to know more about the model that the
method have fitted. We would like some kind of interpretation of what
the underlying methods does, for example:

\begin{itemize}
\tightlist
\item
  what is the mathematical relationship between \(x\) and \(y\) in the
  fitted method?
\item
  how much of the variability in the data is explained by feature \(x\)
  in the fitted method?
\item
  is there an interaction effect between \(x_1\) and \(x_2\) in the
  fitted method?
\end{itemize}

Remark: we want to interpret the fitted method, based on the available
data (but not really interpret directly the data).

We would also like to \emph{explain} the prediction for a given input.

See Chapter 2 of Molnar (2019) on a discussion of
\emph{interpretability}.
\end{block}

\begin{block}{Outline}
\protect\hypertarget{outline}{}
\end{block}

\begin{block}{Reading list}
\protect\hypertarget{reading-list}{}
\begin{itemize}
\tightlist
\item
  (\textbf{Molner2019?}): Chapters 2, 5, 6.1
\item
  4 slide sets from Kjersti Aas

  \begin{itemize}
  \tightlist
  \item
  \item
  \item
  \end{itemize}
\end{itemize}

Supplementary reading is specified for (below) each of the four slide
sets.
\end{block}

\begin{block}{Plan}
\protect\hypertarget{plan}{}
\begin{itemize}
\tightlist
\item
  L12: Two slide sets - Introduction and LIME
\item
  L13: Two slide sets - Shapley values and Counterfactuals
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{L12: Introduction slide set}
\protect\hypertarget{l12-introduction-slide-set}{}
\end{frame}

\begin{frame}{L12: LIME slide set}
\protect\hypertarget{l12-lime-slide-set}{}
\end{frame}

\begin{frame}{L13: Shapley values slide set}
\protect\hypertarget{l13-shapley-values-slide-set}{}
\end{frame}

\begin{frame}{L13: Counterfactuals slide set}
\protect\hypertarget{l13-counterfactuals-slide-set}{}
The two methods can be studied in detail here:

\begin{itemize}
\tightlist
\item
\item
  Dandl et al. (2020)
\end{itemize}

We train a random forest on the Boston dataset: data(``Boston,'' package
= ``MASS'') library(``randomForest'') rf \textless- randomForest(medv
\textasciitilde{} ., data = Boston, ntree = 50) mod \textless-
Predictor\$new(rf, data = Boston)
\end{frame}

\begin{frame}{Compute the accumulated local effects for the first
feature}
\protect\hypertarget{compute-the-accumulated-local-effects-for-the-first-feature}{}
eff \textless-
FeatureEffect\(new(mod, feature = "rm", grid.size = 30) eff\)plot()
\end{frame}

\begin{frame}{Again, but this time with a partial dependence plot and
ice curves}
\protect\hypertarget{again-but-this-time-with-a-partial-dependence-plot-and-ice-curves}{}
eff \textless- FeatureEffect\$new(mod, feature = ``rm,'' method =
``pdp+ice,'' grid.size = 30 ) plot(eff)
\end{frame}

\begin{frame}{Since the result is a ggplot object, you can extend it:}
\protect\hypertarget{since-the-result-is-a-ggplot-object-you-can-extend-it}{}
library(``ggplot2'') plot(eff) + \# Adds a title ggtitle(``Partial
dependence'') + \# Adds original predictions geom\_point( data = Boston,
aes(y = mod\$predict(Boston){[}{[}1{]}{]}, x = rm), color = ``pink,''
size = 0.5 )
\end{frame}

\begin{frame}{If you want to do your own thing, just extract the data:}
\protect\hypertarget{if-you-want-to-do-your-own-thing-just-extract-the-data}{}
eff.dat \textless- eff\$results head(eff.dat)
\end{frame}

\begin{frame}{You can also use the object to ``predict'' the marginal
values.}
\protect\hypertarget{you-can-also-use-the-object-to-predict-the-marginal-values.}{}
eff\(predict(Boston[1:3, ]) # Instead of the entire data.frame, you can also use feature values: eff\)predict(c(5,
6, 7))
\end{frame}

\begin{frame}{You can reuse the pdp object for other features:}
\protect\hypertarget{you-can-reuse-the-pdp-object-for-other-features}{}
eff\$set.feature(``lstat'') plot(eff)
\end{frame}

\begin{frame}{Only plotting the aggregated partial dependence:}
\protect\hypertarget{only-plotting-the-aggregated-partial-dependence}{}
eff \textless-
FeatureEffect\(new(mod, feature = "crim", method = "pdp") eff\)plot()
\end{frame}

\begin{frame}{Only plotting the individual conditional expectation:}
\protect\hypertarget{only-plotting-the-individual-conditional-expectation}{}
eff \textless-
FeatureEffect\(new(mod, feature = "crim", method = "ice") eff\)plot()
\end{frame}

\begin{frame}{Accumulated local effects and partial dependence plots
support up to two}
\protect\hypertarget{accumulated-local-effects-and-partial-dependence-plots-support-up-to-two}{}
\end{frame}

\begin{frame}{features:}
\protect\hypertarget{features}{}
eff \textless- FeatureEffect\$new(mod, feature = c(``crim,'' ``lstat''))
plot(eff)
\end{frame}

\begin{frame}{FeatureEffect plots also works with multiclass
classification}
\protect\hypertarget{featureeffect-plots-also-works-with-multiclass-classification}{}
rf \textless- randomForest(Species \textasciitilde{} ., data = iris,
ntree = 50) mod \textless- Predictor\$new(rf, data = iris, type =
``prob'')
\end{frame}

\begin{frame}{For some models we have to specify additional arguments
for the predict}
\protect\hypertarget{for-some-models-we-have-to-specify-additional-arguments-for-the-predict}{}
\end{frame}

\begin{frame}{function}
\protect\hypertarget{function}{}
plot(FeatureEffect\$new(mod, feature = ``Petal.Width''))
\end{frame}

\begin{frame}{FeatureEffect plots support up to two features:}
\protect\hypertarget{featureeffect-plots-support-up-to-two-features}{}
eff \textless-
FeatureEffect\(new(mod, feature = c("Sepal.Length", "Petal.Length")) eff\)plot()
\end{frame}

\begin{frame}{show where the actual data lies}
\protect\hypertarget{show-where-the-actual-data-lies}{}
eff\$plot(show.data = TRUE)
\end{frame}

\begin{frame}{For multiclass classification models, you can choose to
only show one class:}
\protect\hypertarget{for-multiclass-classification-models-you-can-choose-to-only-show-one-class}{}
mod \textless-
Predictor\(new(rf, data = iris, type = "prob", class = 1) plot(FeatureEffect\)new(mod,
feature = ``Sepal.Length'')) {[}Package iml version 0.10.1 Index{]}
\end{frame}

\begin{frame}{References and further reading}
\protect\hypertarget{references-and-further-reading}{}
\begin{itemize}
\tightlist
\item
  \href{https://www.math.ntnu.no/emner/TMA4268/2019v/11Nnet/11Nnet.html}{Introduction
  to artificial neural networks from TMA4268 in 2019}
\item
  \url{https://youtu.be/aircAruvnKk} from 3BLUE1BROWN - 4 videos - using
  the MNIST-data set as the running example
\item
  Explaining backpropagation
  \url{http://neuralnetworksanddeeplearning.com/chap2.html}
\item
  Look at how the hidden layer behave:
  \url{https://playground.tensorflow.org}
\item
  Friedman, Hastie, and Tibshirani (2001),Chapter 11: Neural Networks
\item
  Efron and Hastie (2016), Chapter 18: Neural Networks and Deep Learning
\end{itemize}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\hypertarget{ref-Dandletal2020}{}%
Dandl, Susanne, Christoph Molnar, Martin Binder, and Bernd Bischl. 2020.
{``Multi-Objective Counterfactual Explanations.''} In \emph{Parallel
Problem Solving from Nature -- PPSN XVI}, edited by Thomas Bäck, Mike
Preuss, André Deutz, Hao Wang, Carola Doerr, Michael Emmerich, and Heike
Trautmann, 448--69. Cham: Springer International Publishing.
\url{https://link.springer.com/content/pdf/10.1007}.

\leavevmode\hypertarget{ref-casi}{}%
Efron, Bradley, and Trevor Hastie. 2016. \emph{Computer Age Statistical
Inference - Algorithms, Evidence, and Data Science}. Cambridge
University Press.

\leavevmode\hypertarget{ref-ESL}{}%
Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. \emph{The
Elements of Statistical Learning}. Vol. 1. Springer series in statistics
New York.

\leavevmode\hypertarget{ref-molnar2019}{}%
Molnar, Christoph. 2019. \emph{Interpretable Machine Learning: A Guide
for Making Black Box Models Explainable}.

\end{CSLReferences}
\end{frame}

\end{document}
